{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from shutil import get_terminal_size\n",
    "import os\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import ast\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\Project\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "goe1 = os.path.join(current_dir, \"goemotion_dataset/goemotions_1.csv\")\n",
    "goe2 = os.path.join(current_dir, \"goemotion_dataset/goemotions_2.csv\")\n",
    "goe3 = os.path.join(current_dir, \"goemotion_dataset/goemotions_3.csv\")\n",
    "\n",
    "\n",
    "goemotion_df1 = pd.read_csv(goe1)\n",
    "goemotion_df2 = pd.read_csv(goe2)\n",
    "goemotion_df3 = pd.read_csv(goe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval',\n",
       "       'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
       "       'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
       "       'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride',\n",
       "       'realization', 'relief', 'remorse', 'sadness', 'surprise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([goemotion_df1, goemotion_df2, goemotion_df3], axis=0, ignore_index=True)\n",
    "\n",
    "df = df[df['example_very_unclear'] == False] # uncler = True인 row 제거\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "df = df.drop(['id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear','neutral'], axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kakao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kakao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kakao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(str)\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.loc[:, 'admiration':'surprise'].sum(axis=1) == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "emotions = df.loc[:, 'admiration':'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, emotions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)  # 최대 5000개의 단어만 고려\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 350\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "\n",
    "y_train_array = y_train.to_numpy()  # 또는 y_train.values\n",
    "y_test_array = y_test.to_numpy()\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_array, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_array, dtype=torch.float32)\n",
    "\n",
    "train_dataset = EmotionDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = EmotionDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Binary Cross Entropy Loss\n",
    "        bce_loss = F.binary_cross_entropy(outputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)  # pt = probability of true class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file, vocab, embedding_dim):\n",
    "\n",
    "    embedding_index = {}\n",
    "\n",
    "    # GloVe 파일 로드\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embedding_index[word] = vector\n",
    "\n",
    "    # 단어 집합 크기 정의\n",
    "    vocab_size = len(vocab) + 1  # 0번 인덱스는 패딩에 사용\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))  # 초기화\n",
    "\n",
    "    # 단어 -> GloVe 벡터 매핑\n",
    "    for word, i in vocab.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  # 매핑된 벡터 추가\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # 최대 단어 수\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = y_train.shape[1]  # 감정 레이블 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe 파일 경로 및 임베딩 차원 설정\n",
    "glove_dir = os.path.join(current_dir, \"glove.6B.100d.txt\")\n",
    "glove_file = glove_dir  # GloVe 임베딩 파일 경로\n",
    "embedding_dim = 100  # GloVe 벡터 차원\n",
    "\n",
    "# 토크나이저에서 단어 집합(vocab) 가져오기\n",
    "vocab = tokenizer.word_index  # 토크나이저의 단어 -> 인덱스 매핑\n",
    "\n",
    "# GloVe 임베딩 매트릭스 생성\n",
    "embedding_matrix = load_glove_embeddings(glove_file, vocab, embedding_dim)\n",
    "\n",
    "# PyTorch 모델 정의\n",
    "class ImprovedDeepLSTMEmotionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=3, dropout=0.3, embedding_matrix=None):\n",
    "        super(ImprovedDeepLSTMEmotionModel, self).__init__()\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))  # GloVe 적용\n",
    "            self.embedding.weight.requires_grad = True  # GloVe 미세조정(fine-tuning) 활성화\n",
    "\n",
    "        # Bi-LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True  # 양방향 LSTM\n",
    "        )\n",
    "\n",
    "        # Conv1D Layer for Feature Extraction\n",
    "        self.conv1d = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout and BatchNorm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Sigmoid for Multi-label Classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Bi-LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out shape: (batch, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Conv1D\n",
    "        conv_out = self.conv1d(lstm_out.permute(0, 2, 1))  # Conv1D requires (batch, channels, seq_len)\n",
    "        conv_out = F.relu(conv_out)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        pooled = torch.mean(conv_out, dim=2)  # shape: (batch, hidden_dim)\n",
    "\n",
    "        # Batch Normalization\n",
    "        normed = self.batch_norm(pooled)\n",
    "\n",
    "        # Fully Connected + Dropout\n",
    "        output = self.fc(self.dropout(normed))\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model = ImprovedDeepLSTMEmotionModel(\n",
    "    vocab_size=len(vocab) + 1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=3,\n",
    "    dropout=0.3,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "\n",
    "# model_dir = os.path.join(current_dir, \"improved_model.pth\")\n",
    "# state_dict = torch.load(model_dir, map_location=torch.device('cpu'))  # state_dict 로드\n",
    "# improved_model.load_state_dict(state_dict)  # state_dict를 모델에 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   2%|▏         | 16/1049 [00:08<09:19,  1.85it/s, loss=0.0416]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m texts, labels \u001b[38;5;241m=\u001b[39m texts\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 옵티마이저 초기화\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mimproved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델 출력\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 역전파\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[62], line 51\u001b[0m, in \u001b[0;36mImprovedDeepLSTMEmotionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Bi-LSTM\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lstm_out shape: (batch, seq_len, hidden_dim * 2)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Conv1D\u001b[39;00m\n\u001b[0;32m     54\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d(lstm_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Conv1D requires (batch, channels, seq_len)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kakao\\Desktop\\Ajou_SocialNetworkAnalysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss(gamma=2, alpha=0.25)\n",
    "optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    improved_model.train()  # 학습 모드 활성화\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for texts, labels in progress_bar:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 옵티마이저 초기화\n",
    "        outputs = improved_model(texts)  # 모델 출력\n",
    "        loss = criterion(outputs, labels)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "# torch.save(improved_model.state_dict(), \"improved_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model.eval()  # 평가 모드 활성화\n",
    "total_loss = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in progress_bar:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = improved_model(texts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probabilities = outputs  # Sigmoid가 이미 적용된 상태\n",
    "        preds = (probabilities > 0.5).float()  # 0.5 임계값으로 이진화\n",
    "\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Confusion Matrix 출력\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "all_preds = torch.cat(all_preds).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emotion_confusion_matrix(true_labels, predicted_labels, label_names):\n",
    "\n",
    "    num_labels = len(label_names)\n",
    "    matrix = np.zeros((num_labels, num_labels), dtype=int)\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        true_indices = np.where(true == 1)[0]  # True labels\n",
    "        pred_indices = np.where(pred == 1)[0]  # Predicted labels\n",
    "\n",
    "        for t in true_indices:\n",
    "            for p in pred_indices:\n",
    "                matrix[t, p] += 1\n",
    "\n",
    "    return pd.DataFrame(matrix, index=label_names, columns=label_names)\n",
    "\n",
    "# 레이블 이름\n",
    "emotion_labels = ['admiration', 'amusement', 'anger', 'annoyance', 'approval',\n",
    "       'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
    "       'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "       'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride',\n",
    "       'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']  # 예시\n",
    "\n",
    "conf_matrix_df = calculate_emotion_confusion_matrix(all_labels, all_preds, emotion_labels)\n",
    "\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Confusion Matrix 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "plt.title(\"Emotion Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Emotion\")\n",
    "plt.ylabel(\"True Emotion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Project\n",
      "    📄 .env\n",
      "    📄 .gitattributes\n",
      "    📄 app.py\n",
      "    📄 classification_report_k_10.csv\n",
      "    📄 classification_report_k_30.csv\n",
      "    📄 df_lstm_training.ipynb\n",
      "    📄 glove.6B.100d.txt\n",
      "    📄 improved_model.pth\n",
      "    📄 improve_lstm.ipynb\n",
      "    📄 ml_k_30_train_text_after_process.csv\n",
      "    📄 ml_train_text_after_process.csv\n",
      "    📄 project_base.ipynb\n",
      "    📄 project_base.py\n",
      "    📄 random_forest_model_textK_10.pkl\n",
      "    📄 random_forest_model_textK_30.pkl\n",
      "    📄 README.md\n",
      "    📄 total_song_lyrics.csv\n",
      "    📄 total_song_lyrics_result.csv\n",
      "    📄 total_song_lyrics_result_add_MLk10.csv\n",
      "    📄 training_ml_model.ipynb\n",
      "    📁 Client\n",
      "        📄 main.html\n",
      "        📄 personer_lyric.html\n",
      "    📁 emotion_lyric\n",
      "        📄 Taylor_Swift\n",
      "        📄 Taylor_Swift.csv\n",
      "    📁 glove\n",
      "        📄 glove.6B.100d.txt\n",
      "        📄 glove.6B.200d.txt\n",
      "        📄 glove.6B.300d.txt\n",
      "        📄 glove.6B.50d.txt\n",
      "    📁 goemotion_dataset\n",
      "        📄 goemotions_1.csv\n",
      "        📄 goemotions_2.csv\n",
      "        📄 goemotions_3.csv\n",
      "    📁 Lib\n",
      "    📁 share\n",
      "        📁 jupyter\n",
      "            📁 kernels\n",
      "                📁 python3\n",
      "                    📄 kernel.json\n",
      "                    📄 logo-32x32.png\n",
      "                    📄 logo-64x64.png\n",
      "                    📄 logo-svg.svg\n",
      "        📁 man\n",
      "            📁 man1\n",
      "                📄 ipython.1\n",
      "                📄 ttx.1\n",
      "    📁 song-lyrics-dataset\n",
      "        📁 csv\n",
      "            📄 ArianaGrande.csv\n",
      "            📄 Beyonce.csv\n",
      "            📄 BillieEilish.csv\n",
      "            📄 BTS.csv\n",
      "            📄 CardiB.csv\n",
      "            📄 CharliePuth.csv\n",
      "            📄 ColdPlay.csv\n",
      "            📄 Drake.csv\n",
      "            📄 DuaLipa.csv\n",
      "            📄 EdSheeran.csv\n",
      "            📄 Eminem.csv\n",
      "            📄 JustinBieber.csv\n",
      "            📄 KatyPerry.csv\n",
      "            📄 Khalid.csv\n",
      "            📄 LadyGaga.csv\n",
      "            📄 Maroon5.csv\n",
      "            📄 NickiMinaj.csv\n",
      "            📄 PostMalone.csv\n",
      "            📄 Rihanna.csv\n",
      "            📄 SelenaGomez.csv\n",
      "            📄 TaylorSwift.csv\n",
      "        📁 json files\n",
      "            📄 Lyrics_ArianaGrande.json\n",
      "            📄 Lyrics_Beyonc.json\n",
      "            📄 Lyrics_BillieEilish.json\n",
      "            📄 Lyrics_BTS.json\n",
      "            📄 Lyrics_CardiB.json\n",
      "            📄 Lyrics_CharliePuth.json\n",
      "            📄 Lyrics_Coldplay.json\n",
      "            📄 Lyrics_Drake.json\n",
      "            📄 Lyrics_DuaLipa.json\n",
      "            📄 Lyrics_EdSheeran.json\n",
      "            📄 Lyrics_Eminem.json\n",
      "            📄 Lyrics_JustinBieber.json\n",
      "            📄 Lyrics_KatyPerry.json\n",
      "            📄 Lyrics_Khalid.json\n",
      "            📄 Lyrics_LadyGaga.json\n",
      "            📄 Lyrics_Maroon5.json\n",
      "            📄 Lyrics_NickiMinaj.json\n",
      "            📄 Lyrics_PostMalone.json\n",
      "            📄 Lyrics_Rihanna.json\n",
      "            📄 Lyrics_SelenaGomez.json\n",
      "            📄 Lyrics_TaylorSwift.json\n"
     ]
    }
   ],
   "source": [
    "def print_directory_tree(startpath, prefix=\"\"):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * level\n",
    "        print(f\"{prefix}{indent}📁 {os.path.basename(root)}\")  # 디렉토리 출력\n",
    "        sub_indent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{prefix}{sub_indent}📄 {f}\")  # 파일 출력\n",
    "\n",
    "# 실행\n",
    "print_directory_tree(current_dir)  # your_directory_path_here에 작업 디렉토리 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
